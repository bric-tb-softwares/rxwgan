{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35772,
     "status": "ok",
     "timestamp": 1611528475221,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "o0pHIjkEFiQO",
    "outputId": "08a41f65-13c9-45a7-8299-fcc0765639a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6088,
     "status": "ok",
     "timestamp": 1611528476710,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "SJngJ7_gFxoM",
    "outputId": "9080b28a-072e-48cb-82a8-05fa222329b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1er27VjG7lJTIAcUBd8fRnfTzF2WjRbDq/BRICS - TB Latente/NBs/wgan\n"
     ]
    }
   ],
   "source": [
    "# %cd '/content/drive/My Drive/BRICS - TB Latente/NBs/wgan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3542,
     "status": "ok",
     "timestamp": 1611528496439,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "InzFsvD4Fx9C"
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import PIL\n",
    "import numpy as np\n",
    "import tensorflow \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from misc import *\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1611528498295,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "_UnyghxjF0oJ"
   },
   "outputs": [],
   "source": [
    "def set_gpu():\n",
    "  gpu_info = !nvidia-smi\n",
    "  gpu_info = '\\n'.join(gpu_info)\n",
    "  if gpu_info.find('failed') >= 0:\n",
    "    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "    print('and then re-execute this cell.')\n",
    "  else:\n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1004,
     "status": "ok",
     "timestamp": 1611528501457,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "jpAs8yRmF7xi"
   },
   "outputs": [],
   "source": [
    "class Wasserstein_GAN(object):\n",
    "\n",
    "  def __init__(self, **kw):\n",
    "    self.images = []\n",
    "    self._height               = retrieve_kw(kw, 'height',               128                                                      )\n",
    "    self._width                = retrieve_kw(kw, 'width',                128                                                      )\n",
    "    self._max_epochs           = retrieve_kw(kw, 'max_epochs',           1000                                                     )\n",
    "    self._batch_size           = retrieve_kw(kw, 'batch_size',           32                                                       )\n",
    "    self._n_features           = retrieve_kw(kw, 'n_features',           NotSet                                                   )\n",
    "    self._n_critic             = retrieve_kw(kw, 'n_critic',               0                                                      )\n",
    "    self._result_file          = retrieve_kw(kw, 'result_file',          \"check_file\"                                             )\n",
    "    self._save_interval        = retrieve_kw(kw, 'save_interval',        100                                                      )\n",
    "    self._use_gradient_penalty = retrieve_kw(kw, 'use_gradient_penalty', True                                                     )\n",
    "    self._verbose              = retrieve_kw(kw, 'verbose',              True                                                     )\n",
    "    self._gen_opt              = retrieve_kw(kw, 'gen_opt',              tf.optimizers.Adam(lr=1e-4, beta_1=0.5, decay=1e-4 )     )\n",
    "    self._critic_opt           = retrieve_kw(kw, 'critic_opt',           tf.optimizers.Adam(lr=1e-4, beta_1=0.5, decay=1e-4 )     )\n",
    "    self._tf_call_kw           = retrieve_kw(kw, 'tf_call_kw',           {}                                                       )\n",
    "    self._grad_weight          = tf.constant( retrieve_kw(kw, 'grad_weight',          10.0                                      ) )\n",
    "    self._latent_dim           = tf.constant( retrieve_kw(kw, 'latent_dim',           100                                       ) )\n",
    "    self._leaky_relu_alpha     = retrieve_kw(kw, 'leaky_relu_alpha',     0.3                                                    )\n",
    "    \n",
    "    # Initialize discriminator and generator networks\n",
    "    self.critic = self._build_critic()\n",
    "    self.generator = self._build_generator()\n",
    "\n",
    "  def latent_dim(self):\n",
    "    return self._latent_dim\n",
    "\n",
    "  @tf.function\n",
    "  def latent_log_prob(self, latent):\n",
    "    prior = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(self._latent_dim),\n",
    "                                                     scale_diag=tf.ones(self._latent_dim))\n",
    "    return prior.log_prob(latent)\n",
    "\n",
    "  @tf.function\n",
    "  def wasserstein_loss(self, y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true) - tf.reduce_mean(y_pred)\n",
    "\n",
    "  @tf.function\n",
    "  def sample_latent_data(self, nsamples):\n",
    "    return tf.random.normal((nsamples, self._latent_dim))\n",
    "\n",
    "  @tf.function\n",
    "  def transform(self, latent):\n",
    "    return self.generator( latent, **self._tf_call_kw)\n",
    "\n",
    "  @tf.function\n",
    "  def generate(self, nsamples):\n",
    "    return self.transform( self.sample_latent_data( nsamples ))\n",
    "\n",
    "  def train(self, train_data, name_file):\n",
    "    if self._n_features is NotSet:\n",
    "      self._n_features = train_data.shape[1]\n",
    "    if self._verbose: print('Number of features is %d.' % self._n_features )\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    n_gpus = len(gpus)\n",
    "    if self._verbose: print('This machine has %i GPUs.' % n_gpus)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices( train_data ).batch( self._batch_size, drop_remainder = True )\n",
    "\n",
    "    # checkpoint for the model\n",
    "    checkpoint_maker = tf.train.Checkpoint(generator_optimizer=self._gen_opt,\n",
    "        discriminator_optimizer=self._critic_opt,\n",
    "        generator=self.generator,\n",
    "        discriminator=self.critic\n",
    "    )if self._result_file else None\n",
    "\n",
    "    # containers for losses\n",
    "    losses = {'critic': [], 'generator': [], 'regularizer': []}\n",
    "    critic_acc = []\n",
    "\n",
    "    #reduce_lr = ReduceLROnPlateau(monitor='loss', patience=100, mode='auto',\n",
    "    #                              factor=factor, cooldown=0, min_lr=1e-4, verbose=2)\n",
    "    #model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=callback_list,\n",
    "    #          class_weight=class_weight, verbose=2, validation_data=(X_test, y_test))\n",
    "\n",
    "    updates = 0; batches = 0;\n",
    "    for epoch in range(self._max_epochs):\n",
    "      for sample_batch in train_dataset:\n",
    "        if self._n_critic and (updates % self._n_critic):\n",
    "          # Update only critic\n",
    "          critic_loss, reg_loss, gen_loss = self._train_critic(sample_batch) + (np.nan,)\n",
    "        if not(self._n_critic) or not(updates % self._n_critic):\n",
    "          # Update critic and generator\n",
    "          critic_loss, gen_loss, reg_loss = self._train_step(sample_batch)\n",
    "        losses['critic'].append(critic_loss)\n",
    "        losses['generator'].append(gen_loss)\n",
    "        losses['regularizer'].append(reg_loss)\n",
    "        updates += 1\n",
    "        #perc = np.around(100*epoch/self._max_epochs, decimals=1)\n",
    "        \n",
    "        # Save current model\n",
    "        #if checkpoint_maker and not(updates % self._save_interval):\n",
    "        #  checkpoint_maker.save(file_prefix=self._result_file)\n",
    "        #  pass\n",
    "        # Print logging information\n",
    "        if self._verbose and not(updates % self._save_interval):\n",
    "          perc = np.around(100*epoch/self._max_epochs, decimals=1)\n",
    "          print('Epoch: %i. Updates %i. Training %1.1f%% complete. Critic_loss: %.3f. Gen_loss: %.3f. Regularizer: %.3f'\n",
    "               % (epoch, updates, perc, critic_loss, gen_loss, reg_loss ))\n",
    "          \n",
    "    checkpoint_maker.save(file_prefix=name_file)\n",
    "    self.save( name_file, True )\n",
    "    return losses\n",
    "\n",
    "  def save(self, name_file, overwrite = False ):\n",
    "    self.generator.save_weights( \"./\" + name_file + \"/\" + name_file + '_generator', overwrite )\n",
    "    self.critic.save_weights( \"./\" + name_file + \"/\" + name_file + '_critic', overwrite )\n",
    "\n",
    "  def load(self, path ):\n",
    "    self.generator.load_weights( path + '_generator' )\n",
    "    self.critic.load_weights( path + '_critic' )\n",
    "\n",
    "  def _build_critic(self):\n",
    "    ip = layers.Input(shape=(self._height,self._width,1))\n",
    "    # TODO Add other normalization scheme as mentioned in the article\n",
    "    # Input (None, 3^2*2^5 = 1 day = 288 samples, 1)\n",
    "    y = layers.Conv2D(256, (5,5), strides=(2,2), padding='same', kernel_initializer='he_uniform', data_format='channels_last', input_shape=(self._height,self._width,1))(ip)\n",
    "    #y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Dropout(rate=0.3, seed=1)(y)\n",
    "    # Output (None, 3^2*2^3, 64)\n",
    "    y = layers.Conv2D(128, (5,5), strides=(2,2), padding='same', kernel_initializer='he_uniform')(y)\n",
    "    #y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Dropout(rate=0.3, seed=1)(y)\n",
    "    # Output (None, 3^2*2^3, 64)\n",
    "    y = layers.Conv2D(64, (5,5), strides=(2,2), padding='same', kernel_initializer='he_uniform')(y)\n",
    "    #y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation('relu')(y)\n",
    "    y = layers.Dropout(rate=0.3, seed=1)(y)\n",
    "    # Output (None, 3^2*2, 128)\n",
    "    y = layers.Flatten()(y)\n",
    "    # Output (None, 3*256)\n",
    "    #out = layers.Dense(nb_class, activation='sigmoid')(y)\n",
    "    out = layers.Dense(1, activation='linear')(y)\n",
    "    # Output (None, 1)\n",
    "    model = tf.keras.Model(ip, out)\n",
    "    if self._verbose: model.summary()\n",
    "    model.compile()\n",
    "    #y = layers.GlobalAveragePooling1D()(y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "  def _build_generator(self):\n",
    "    ip = layers.Input(shape=(self._latent_dim,))\n",
    "    # Input (None, latent space (100?) )\n",
    "    y = layers.Dense(units=16*16*32, input_shape=(self._latent_dim,))(ip)\n",
    "    # Output (None, 64*3^2 )\n",
    "    y = layers.Reshape(target_shape=(16,16, 32))(y)\n",
    "    #y = layers.BatchNormalization()(y)\n",
    "    #y = layers.LeakyReLU(alpha=self._leaky_relu_alpha)(y)\n",
    "    #y = layers.UpSampling1D()(y)\n",
    "    # Output (None, 3^2*2, 64)\n",
    "    y = layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU(alpha=self._leaky_relu_alpha)(y)\n",
    "    y = layers.Dropout(rate=0.3)(y)\n",
    "    #y = layers.UpSampling1D(size=2*2)(y)\n",
    "    # Output (None, 3^2*2^3, 128)\n",
    "    y = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU(alpha=self._leaky_relu_alpha)(y)\n",
    "    y = layers.Dropout(rate=0.3)(y)\n",
    "    #y = layers.UpSampling1D(size=2*2)(y)\n",
    "    # Output (None, 3^2*2^5, 256)\n",
    "    y = layers.Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', kernel_initializer='he_uniform')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU(alpha=self._leaky_relu_alpha)(y)\n",
    "    y = layers.Dropout(rate=0.3)(y)\n",
    "    # Output (None, 3^2*2^5, 64)\n",
    "    out = layers.Conv2DTranspose(1, (4,4), strides=(1,1), padding='same', kernel_initializer='he_uniform', activation = 'tanh')(y)\n",
    "    # Output (None, 3^2*2^5, 1)\n",
    "    model = tf.keras.Model(ip, out)\n",
    "    if self._verbose: model.summary()\n",
    "    model.compile()\n",
    "    return model\n",
    "\n",
    "  @tf.function\n",
    "  def _gradient_penalty(self, x, x_hat):\n",
    "    epsilon = tf.random.uniform((self._batch_size, self._height, self._width, 1), 0.0, 1.0)\n",
    "    u_hat = epsilon * x + (1 - epsilon) * x_hat\n",
    "    with tf.GradientTape() as penalty_tape:\n",
    "      penalty_tape.watch(u_hat)\n",
    "      func = self.critic(u_hat)\n",
    "    grads = penalty_tape.gradient(func, u_hat)\n",
    "    norm_grads = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "    regularizer = tf.math.square( tf.reduce_mean((norm_grads - 1) ) )\n",
    "    return regularizer\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def _get_critic_output( self, samples, fake_samples ):\n",
    "    # calculate critic outputs\n",
    "    real_output = self.critic(samples, **self._tf_call_kw)\n",
    "    fake_output = self.critic(fake_samples, **self._tf_call_kw)\n",
    "    return real_output, fake_output\n",
    "\n",
    "  @tf.function\n",
    "  def _get_critic_loss( self, samples, fake_samples, real_output, fake_output ):\n",
    "    grad_regularizer_loss = tf.multiply(self._grad_weight, self._gradient_penalty(samples, fake_samples)) if self._use_gradient_penalty else 0\n",
    "    critic_loss = tf.add( self.wasserstein_loss(real_output, fake_output), grad_regularizer_loss )\n",
    "    return critic_loss, grad_regularizer_loss\n",
    "\n",
    "  def _get_gen_loss( self, fake_samples, fake_output ):\n",
    "    gen_loss = tf.reduce_mean(fake_output)\n",
    "    return gen_loss\n",
    "\n",
    "  def _apply_critic_update( self, critic_tape, critic_loss ):\n",
    "    critic_grads = critic_tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "    self._critic_opt.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "    return\n",
    "\n",
    "  def _apply_gen_update( self, gen_tape, gen_loss):\n",
    "    gen_grads = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "    self._gen_opt.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "    return\n",
    "\n",
    "  @tf.function\n",
    "  def _train_critic(self, samples):\n",
    "    with tf.GradientTape() as critic_tape:\n",
    "      fake_samples = self.generate( self._batch_size )\n",
    "      real_output, fake_output = self._get_critic_output( samples, fake_samples )\n",
    "      critic_loss, grad_regularizer_loss = self._get_critic_loss( samples, fake_samples, real_output, fake_output)\n",
    "    # critic_tape\n",
    "    self._apply_critic_update( critic_tape, critic_loss )\n",
    "    return critic_loss, grad_regularizer_loss\n",
    "\n",
    "  @tf.function\n",
    "  def _train_step(self, samples):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as critic_tape:\n",
    "      fake_samples = self.generate( self._batch_size )\n",
    "      real_output, fake_output = self._get_critic_output( samples, fake_samples )\n",
    "      critic_loss, critic_regularizer = self._get_critic_loss( samples, fake_samples, real_output, fake_output)\n",
    "      gen_loss = self._get_gen_loss( fake_samples, fake_output )\n",
    "    # gen_tape, critic_tape\n",
    "    self._apply_critic_update( critic_tape, critic_loss )\n",
    "    self._apply_gen_update( gen_tape, gen_loss )\n",
    "    return critic_loss, gen_loss, critic_regularizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1611528507369,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "mB_HwENGF9em"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(BASE_DIR):\n",
    "  HEIGHT      = 128\n",
    "  WIDTH       = 128\n",
    "  SIZE        = (WIDTH,HEIGHT)\n",
    "  IMAGES_PATH = [os.path.join(BASE_DIR, filename) for filename in os.listdir(BASE_DIR) if \".png\" in filename]\n",
    "  NUM_IMAGES  = len(IMAGES_PATH)\n",
    "  data_images = []                                                                                            \n",
    "  y_label     = []\n",
    "\n",
    "  # for every image path in the list of IMAGES_PATH get an image in the directory location, resize and convert to grayscale and transform to array and append in the data_images\n",
    "  for path in IMAGES_PATH:\n",
    "    img  = Image.open(path).resize(SIZE).convert('L')\n",
    "    data = asarray(img)\n",
    "    data_images.append(data)\n",
    "    y_label.append(path[-5])\n",
    "\n",
    "  data_images = np.array(data_images,dtype='f')\n",
    "  data_images.resize((NUM_IMAGES,HEIGHT,WIDTH,1))\n",
    "  data_images = data_images/255.0\n",
    "  y_label = np.array(y_label)\n",
    "  \n",
    "  return data_images,y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 340724,
     "status": "ok",
     "timestamp": 1611528849486,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "fy9hF3vFF-At"
   },
   "outputs": [],
   "source": [
    "# Preproccess dataset with chest x-ray images of China with the pattern and split into train and validation data, that is a list with the images witht the preset proportion\n",
    "# TRAIN_IMAGES_DIR = '/content/drive/My Drive/BRICS - TB Latente/Dados/ChinaSet_AllFiles/CXR_png/'\n",
    "# data_base,y_label = preprocess_data(TRAIN_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1611529144066,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "Af1_IV4a8Va7",
    "outputId": "aba289fe-d211-42cb-d307-d81927e02852"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skf = StratifiedKFold(n_splits=10, random_state=13)\r\n",
    "# skf.get_n_splits(data_base,y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 718,
     "status": "ok",
     "timestamp": 1611529146773,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "bsm3PEKI_GSv",
    "outputId": "acbf75f5-4602-426a-ab78-568e33cc146f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 595 TEST: 67\n",
      "TRAIN: 595 TEST: 67\n",
      "TRAIN: 596 TEST: 66\n",
      "TRAIN: 596 TEST: 66\n",
      "TRAIN: 596 TEST: 66\n",
      "TRAIN: 596 TEST: 66\n",
      "TRAIN: 596 TEST: 66\n",
      "TRAIN: 596 TEST: 66\n",
      "TRAIN: 596 TEST: 66\n",
      "TRAIN: 596 TEST: 66\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test = [],[]\r\n",
    "# for train_index, test_index in skf.split(data_base,y_label):\r\n",
    "#  print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\r\n",
    "#  X_train.append(np.squeeze(np.array([data_base[train_index]]),axis=0))\r\n",
    "#  X_test.append(np.squeeze(np.array([data_base[test_index]]),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6290,
     "status": "ok",
     "timestamp": 1611529159551,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "2vkXONgqGCJ3",
    "outputId": "10ac22a5-cd66-430b-95be-d497c7664ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 256)       6656      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       819328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        204864    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 16385     \n",
      "=================================================================\n",
      "Total params: 1,047,233\n",
      "Trainable params: 1,047,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8192)              827392    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 64)        32832     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 128)       131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 128, 128, 256)     524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128, 128, 256)     1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 128, 128, 1)       4097      \n",
      "=================================================================\n",
      "Total params: 1,521,857\n",
      "Trainable params: 1,520,961\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# wass_gan = Wasserstein_GAN()        # Create an object of WGAN \n",
    "# wass_gan.load('chkp_2/check_file')  # Load the weights of the models pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1546757,
     "status": "ok",
     "timestamp": 1611538412788,
     "user": {
      "displayName": "Guilherme Goldman da Silva",
      "photoUrl": "",
      "userId": "04497542779516175526"
     },
     "user_tz": 180
    },
    "id": "zGgCx1UWGDDw",
    "outputId": "75048368-dae8-42ea-e13f-b4a706b3a4e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features is 128.\n",
      "This machine has 1 GPUs.\n",
      "Epoch: 5. Updates 100. Training 0.5% complete. Critic_loss: 3.986. Gen_loss: -110.514. Regularizer: 9.374\n",
      "Epoch: 11. Updates 200. Training 1.1% complete. Critic_loss: 0.819. Gen_loss: 106.696. Regularizer: 6.189\n",
      "Epoch: 16. Updates 300. Training 1.6% complete. Critic_loss: -8.606. Gen_loss: -55.763. Regularizer: 1.699\n",
      "Epoch: 22. Updates 400. Training 2.2% complete. Critic_loss: -8.608. Gen_loss: 74.708. Regularizer: 1.282\n",
      "Epoch: 27. Updates 500. Training 2.7% complete. Critic_loss: -7.671. Gen_loss: -7.428. Regularizer: 0.148\n",
      "Epoch: 33. Updates 600. Training 3.3% complete. Critic_loss: -7.784. Gen_loss: -30.547. Regularizer: 0.349\n",
      "Epoch: 38. Updates 700. Training 3.8% complete. Critic_loss: -9.067. Gen_loss: 63.242. Regularizer: 1.228\n",
      "Epoch: 44. Updates 800. Training 4.4% complete. Critic_loss: -7.969. Gen_loss: 22.257. Regularizer: 0.073\n",
      "Epoch: 49. Updates 900. Training 4.9% complete. Critic_loss: -4.356. Gen_loss: 4.018. Regularizer: 0.074\n",
      "Epoch: 55. Updates 1000. Training 5.5% complete. Critic_loss: 2.824. Gen_loss: -55.569. Regularizer: 0.464\n",
      "Epoch: 61. Updates 1100. Training 6.1% complete. Critic_loss: -5.177. Gen_loss: -17.870. Regularizer: 0.552\n",
      "Epoch: 66. Updates 1200. Training 6.6% complete. Critic_loss: -2.563. Gen_loss: 13.338. Regularizer: 0.636\n",
      "Epoch: 72. Updates 1300. Training 7.2% complete. Critic_loss: -2.015. Gen_loss: 10.504. Regularizer: 0.474\n",
      "Epoch: 77. Updates 1400. Training 7.7% complete. Critic_loss: -3.823. Gen_loss: 7.952. Regularizer: 0.575\n",
      "Epoch: 83. Updates 1500. Training 8.3% complete. Critic_loss: -3.717. Gen_loss: 5.153. Regularizer: 0.091\n",
      "Epoch: 88. Updates 1600. Training 8.8% complete. Critic_loss: -2.861. Gen_loss: 7.516. Regularizer: 0.087\n",
      "Epoch: 94. Updates 1700. Training 9.4% complete. Critic_loss: -2.670. Gen_loss: 7.184. Regularizer: 0.097\n",
      "Epoch: 99. Updates 1800. Training 9.9% complete. Critic_loss: -3.169. Gen_loss: 14.189. Regularizer: 0.507\n",
      "Epoch: 105. Updates 1900. Training 10.5% complete. Critic_loss: -5.361. Gen_loss: 5.124. Regularizer: 0.265\n",
      "Epoch: 111. Updates 2000. Training 11.1% complete. Critic_loss: -3.190. Gen_loss: 6.497. Regularizer: 0.111\n",
      "Epoch: 116. Updates 2100. Training 11.6% complete. Critic_loss: -2.861. Gen_loss: 15.007. Regularizer: 0.750\n",
      "Epoch: 122. Updates 2200. Training 12.2% complete. Critic_loss: -1.935. Gen_loss: -3.169. Regularizer: 0.027\n",
      "Epoch: 127. Updates 2300. Training 12.7% complete. Critic_loss: -0.647. Gen_loss: 23.027. Regularizer: 0.239\n",
      "Epoch: 133. Updates 2400. Training 13.3% complete. Critic_loss: -3.122. Gen_loss: 2.736. Regularizer: 0.058\n",
      "Epoch: 138. Updates 2500. Training 13.8% complete. Critic_loss: -3.345. Gen_loss: 13.223. Regularizer: 0.345\n",
      "Epoch: 144. Updates 2600. Training 14.4% complete. Critic_loss: -3.501. Gen_loss: 8.535. Regularizer: 0.190\n",
      "Epoch: 149. Updates 2700. Training 14.9% complete. Critic_loss: -4.090. Gen_loss: 15.341. Regularizer: 0.515\n",
      "Epoch: 155. Updates 2800. Training 15.5% complete. Critic_loss: -3.063. Gen_loss: 2.122. Regularizer: 0.378\n",
      "Epoch: 161. Updates 2900. Training 16.1% complete. Critic_loss: -2.343. Gen_loss: 6.630. Regularizer: 0.105\n",
      "Epoch: 166. Updates 3000. Training 16.6% complete. Critic_loss: -1.342. Gen_loss: 6.897. Regularizer: 0.102\n",
      "Epoch: 172. Updates 3100. Training 17.2% complete. Critic_loss: -1.758. Gen_loss: 7.318. Regularizer: 0.127\n",
      "Epoch: 177. Updates 3200. Training 17.7% complete. Critic_loss: -1.611. Gen_loss: 3.190. Regularizer: 0.460\n",
      "Epoch: 183. Updates 3300. Training 18.3% complete. Critic_loss: -3.133. Gen_loss: 4.264. Regularizer: 0.101\n",
      "Epoch: 188. Updates 3400. Training 18.8% complete. Critic_loss: -2.512. Gen_loss: 11.327. Regularizer: 0.054\n",
      "Epoch: 194. Updates 3500. Training 19.4% complete. Critic_loss: -3.164. Gen_loss: 10.313. Regularizer: 0.213\n",
      "Epoch: 199. Updates 3600. Training 19.9% complete. Critic_loss: -1.951. Gen_loss: 7.210. Regularizer: 0.086\n",
      "Epoch: 205. Updates 3700. Training 20.5% complete. Critic_loss: -3.266. Gen_loss: 5.609. Regularizer: 0.377\n",
      "Epoch: 211. Updates 3800. Training 21.1% complete. Critic_loss: -2.856. Gen_loss: 5.279. Regularizer: 0.010\n",
      "Epoch: 216. Updates 3900. Training 21.6% complete. Critic_loss: 3.193. Gen_loss: 4.349. Regularizer: 0.302\n",
      "Epoch: 222. Updates 4000. Training 22.2% complete. Critic_loss: -2.472. Gen_loss: 9.822. Regularizer: 0.312\n",
      "Epoch: 227. Updates 4100. Training 22.7% complete. Critic_loss: -1.938. Gen_loss: 5.399. Regularizer: 0.097\n",
      "Epoch: 233. Updates 4200. Training 23.3% complete. Critic_loss: -2.150. Gen_loss: 8.017. Regularizer: 0.170\n",
      "Epoch: 238. Updates 4300. Training 23.8% complete. Critic_loss: -2.894. Gen_loss: 9.320. Regularizer: 0.129\n",
      "Epoch: 244. Updates 4400. Training 24.4% complete. Critic_loss: -2.029. Gen_loss: 2.564. Regularizer: 0.163\n",
      "Epoch: 249. Updates 4500. Training 24.9% complete. Critic_loss: -2.611. Gen_loss: 7.852. Regularizer: 0.185\n",
      "Epoch: 255. Updates 4600. Training 25.5% complete. Critic_loss: -2.523. Gen_loss: 5.314. Regularizer: 0.021\n",
      "Epoch: 261. Updates 4700. Training 26.1% complete. Critic_loss: -2.521. Gen_loss: 5.536. Regularizer: 0.126\n",
      "Epoch: 266. Updates 4800. Training 26.6% complete. Critic_loss: -0.274. Gen_loss: 3.085. Regularizer: 0.175\n",
      "Epoch: 272. Updates 4900. Training 27.2% complete. Critic_loss: -1.941. Gen_loss: 7.237. Regularizer: 0.079\n",
      "Epoch: 277. Updates 5000. Training 27.7% complete. Critic_loss: -0.036. Gen_loss: 5.315. Regularizer: 0.176\n",
      "Epoch: 283. Updates 5100. Training 28.3% complete. Critic_loss: -1.359. Gen_loss: 4.548. Regularizer: 0.120\n",
      "Epoch: 288. Updates 5200. Training 28.8% complete. Critic_loss: -1.735. Gen_loss: -1.097. Regularizer: 0.340\n",
      "Epoch: 294. Updates 5300. Training 29.4% complete. Critic_loss: -2.308. Gen_loss: 4.512. Regularizer: 0.039\n",
      "Epoch: 299. Updates 5400. Training 29.9% complete. Critic_loss: -1.740. Gen_loss: -1.402. Regularizer: 0.383\n",
      "Epoch: 305. Updates 5500. Training 30.5% complete. Critic_loss: -2.561. Gen_loss: 5.755. Regularizer: 0.067\n",
      "Epoch: 311. Updates 5600. Training 31.1% complete. Critic_loss: -2.116. Gen_loss: 4.313. Regularizer: 0.017\n",
      "Epoch: 316. Updates 5700. Training 31.6% complete. Critic_loss: -0.958. Gen_loss: 3.231. Regularizer: 0.100\n",
      "Epoch: 322. Updates 5800. Training 32.2% complete. Critic_loss: -1.538. Gen_loss: 2.290. Regularizer: 0.088\n",
      "Epoch: 327. Updates 5900. Training 32.7% complete. Critic_loss: -1.111. Gen_loss: 3.093. Regularizer: 0.085\n",
      "Epoch: 333. Updates 6000. Training 33.3% complete. Critic_loss: -1.025. Gen_loss: 1.696. Regularizer: 0.011\n",
      "Epoch: 338. Updates 6100. Training 33.8% complete. Critic_loss: -1.917. Gen_loss: 3.992. Regularizer: 0.167\n",
      "Epoch: 344. Updates 6200. Training 34.4% complete. Critic_loss: -2.085. Gen_loss: 3.348. Regularizer: 0.008\n",
      "Epoch: 349. Updates 6300. Training 34.9% complete. Critic_loss: -1.760. Gen_loss: 6.389. Regularizer: 0.076\n",
      "Epoch: 355. Updates 6400. Training 35.5% complete. Critic_loss: -2.214. Gen_loss: 3.941. Regularizer: 0.124\n",
      "Epoch: 361. Updates 6500. Training 36.1% complete. Critic_loss: -2.342. Gen_loss: 2.325. Regularizer: 0.076\n",
      "Epoch: 366. Updates 6600. Training 36.6% complete. Critic_loss: -1.371. Gen_loss: 2.545. Regularizer: 0.068\n",
      "Epoch: 372. Updates 6700. Training 37.2% complete. Critic_loss: -1.779. Gen_loss: 2.229. Regularizer: 0.059\n",
      "Epoch: 377. Updates 6800. Training 37.7% complete. Critic_loss: -1.353. Gen_loss: -1.507. Regularizer: 0.228\n",
      "Epoch: 383. Updates 6900. Training 38.3% complete. Critic_loss: -2.242. Gen_loss: 3.336. Regularizer: 0.022\n",
      "Epoch: 388. Updates 7000. Training 38.8% complete. Critic_loss: -1.982. Gen_loss: 3.586. Regularizer: 0.097\n",
      "Epoch: 394. Updates 7100. Training 39.4% complete. Critic_loss: -1.152. Gen_loss: 0.907. Regularizer: 0.081\n",
      "Epoch: 399. Updates 7200. Training 39.9% complete. Critic_loss: -1.996. Gen_loss: 2.700. Regularizer: 0.050\n",
      "Epoch: 405. Updates 7300. Training 40.5% complete. Critic_loss: -2.124. Gen_loss: 0.166. Regularizer: 0.089\n",
      "Epoch: 411. Updates 7400. Training 41.1% complete. Critic_loss: -2.345. Gen_loss: 3.709. Regularizer: 0.041\n",
      "Epoch: 416. Updates 7500. Training 41.6% complete. Critic_loss: -0.978. Gen_loss: 0.412. Regularizer: 0.080\n",
      "Epoch: 422. Updates 7600. Training 42.2% complete. Critic_loss: -0.503. Gen_loss: 1.053. Regularizer: 0.005\n",
      "Epoch: 427. Updates 7700. Training 42.7% complete. Critic_loss: -1.436. Gen_loss: 1.807. Regularizer: 0.077\n",
      "Epoch: 433. Updates 7800. Training 43.3% complete. Critic_loss: -1.250. Gen_loss: 1.997. Regularizer: 0.095\n",
      "Epoch: 438. Updates 7900. Training 43.8% complete. Critic_loss: -1.871. Gen_loss: 2.639. Regularizer: 0.164\n",
      "Epoch: 444. Updates 8000. Training 44.4% complete. Critic_loss: -1.735. Gen_loss: 1.641. Regularizer: 0.003\n",
      "Epoch: 449. Updates 8100. Training 44.9% complete. Critic_loss: -1.836. Gen_loss: 1.637. Regularizer: 0.062\n",
      "Epoch: 455. Updates 8200. Training 45.5% complete. Critic_loss: -2.828. Gen_loss: 2.651. Regularizer: 0.020\n",
      "Epoch: 461. Updates 8300. Training 46.1% complete. Critic_loss: -2.259. Gen_loss: 2.845. Regularizer: 0.082\n",
      "Epoch: 466. Updates 8400. Training 46.6% complete. Critic_loss: -1.363. Gen_loss: 1.625. Regularizer: 0.152\n",
      "Epoch: 472. Updates 8500. Training 47.2% complete. Critic_loss: -1.320. Gen_loss: 1.843. Regularizer: 0.053\n",
      "Epoch: 477. Updates 8600. Training 47.7% complete. Critic_loss: -1.433. Gen_loss: 1.371. Regularizer: 0.071\n",
      "Epoch: 483. Updates 8700. Training 48.3% complete. Critic_loss: -1.367. Gen_loss: 0.975. Regularizer: 0.021\n",
      "Epoch: 488. Updates 8800. Training 48.8% complete. Critic_loss: -1.021. Gen_loss: -1.346. Regularizer: 0.065\n",
      "Epoch: 494. Updates 8900. Training 49.4% complete. Critic_loss: -0.777. Gen_loss: 0.599. Regularizer: 0.042\n",
      "Epoch: 499. Updates 9000. Training 49.9% complete. Critic_loss: -2.434. Gen_loss: -0.524. Regularizer: 0.108\n",
      "Epoch: 505. Updates 9100. Training 50.5% complete. Critic_loss: -1.165. Gen_loss: 1.597. Regularizer: 0.098\n",
      "Epoch: 511. Updates 9200. Training 51.1% complete. Critic_loss: -1.996. Gen_loss: 2.916. Regularizer: 0.014\n",
      "Epoch: 516. Updates 9300. Training 51.6% complete. Critic_loss: -1.727. Gen_loss: 4.652. Regularizer: 0.066\n",
      "Epoch: 522. Updates 9400. Training 52.2% complete. Critic_loss: -1.436. Gen_loss: 0.845. Regularizer: 0.036\n",
      "Epoch: 527. Updates 9500. Training 52.7% complete. Critic_loss: -1.472. Gen_loss: 0.778. Regularizer: 0.094\n",
      "Epoch: 533. Updates 9600. Training 53.3% complete. Critic_loss: -1.404. Gen_loss: 0.978. Regularizer: 0.039\n",
      "Epoch: 538. Updates 9700. Training 53.8% complete. Critic_loss: -1.818. Gen_loss: 3.960. Regularizer: 0.179\n",
      "Epoch: 544. Updates 9800. Training 54.4% complete. Critic_loss: -1.527. Gen_loss: 2.104. Regularizer: 0.004\n",
      "Epoch: 549. Updates 9900. Training 54.9% complete. Critic_loss: -1.647. Gen_loss: 1.456. Regularizer: 0.075\n",
      "Epoch: 555. Updates 10000. Training 55.5% complete. Critic_loss: -1.952. Gen_loss: 1.007. Regularizer: 0.026\n",
      "Epoch: 561. Updates 10100. Training 56.1% complete. Critic_loss: -1.810. Gen_loss: 1.410. Regularizer: 0.028\n",
      "Epoch: 566. Updates 10200. Training 56.6% complete. Critic_loss: -0.688. Gen_loss: 0.522. Regularizer: 0.161\n",
      "Epoch: 572. Updates 10300. Training 57.2% complete. Critic_loss: -1.516. Gen_loss: 1.330. Regularizer: 0.063\n",
      "Epoch: 577. Updates 10400. Training 57.7% complete. Critic_loss: -1.087. Gen_loss: 0.447. Regularizer: 0.088\n",
      "Epoch: 583. Updates 10500. Training 58.3% complete. Critic_loss: -1.201. Gen_loss: -0.026. Regularizer: 0.047\n",
      "Epoch: 588. Updates 10600. Training 58.8% complete. Critic_loss: -1.566. Gen_loss: -0.007. Regularizer: 0.071\n",
      "Epoch: 594. Updates 10700. Training 59.4% complete. Critic_loss: -1.509. Gen_loss: -0.214. Regularizer: 0.049\n",
      "Epoch: 599. Updates 10800. Training 59.9% complete. Critic_loss: -2.212. Gen_loss: 0.290. Regularizer: 0.057\n",
      "Epoch: 605. Updates 10900. Training 60.5% complete. Critic_loss: -1.716. Gen_loss: 0.034. Regularizer: 0.084\n",
      "Epoch: 611. Updates 11000. Training 61.1% complete. Critic_loss: -1.414. Gen_loss: 0.703. Regularizer: 0.046\n",
      "Epoch: 616. Updates 11100. Training 61.6% complete. Critic_loss: -1.193. Gen_loss: -0.052. Regularizer: 0.104\n",
      "Epoch: 622. Updates 11200. Training 62.2% complete. Critic_loss: -1.436. Gen_loss: 0.611. Regularizer: 0.043\n",
      "Epoch: 627. Updates 11300. Training 62.7% complete. Critic_loss: -1.262. Gen_loss: -0.660. Regularizer: 0.043\n",
      "Epoch: 633. Updates 11400. Training 63.3% complete. Critic_loss: -1.160. Gen_loss: 0.732. Regularizer: 0.018\n",
      "Epoch: 638. Updates 11500. Training 63.8% complete. Critic_loss: -1.987. Gen_loss: 0.572. Regularizer: 0.088\n",
      "Epoch: 644. Updates 11600. Training 64.4% complete. Critic_loss: -1.243. Gen_loss: -0.078. Regularizer: 0.013\n",
      "Epoch: 649. Updates 11700. Training 64.9% complete. Critic_loss: -2.049. Gen_loss: -0.112. Regularizer: 0.041\n",
      "Epoch: 655. Updates 11800. Training 65.5% complete. Critic_loss: -1.461. Gen_loss: -2.494. Regularizer: 0.048\n",
      "Epoch: 661. Updates 11900. Training 66.1% complete. Critic_loss: -1.697. Gen_loss: 0.396. Regularizer: 0.004\n",
      "Epoch: 666. Updates 12000. Training 66.6% complete. Critic_loss: -1.440. Gen_loss: -0.817. Regularizer: 0.139\n",
      "Epoch: 672. Updates 12100. Training 67.2% complete. Critic_loss: -1.027. Gen_loss: -0.072. Regularizer: 0.034\n",
      "Epoch: 677. Updates 12200. Training 67.7% complete. Critic_loss: -1.323. Gen_loss: -0.729. Regularizer: 0.054\n",
      "Epoch: 683. Updates 12300. Training 68.3% complete. Critic_loss: -1.525. Gen_loss: 0.939. Regularizer: 0.037\n",
      "Epoch: 688. Updates 12400. Training 68.8% complete. Critic_loss: -1.449. Gen_loss: -1.083. Regularizer: 0.157\n",
      "Epoch: 694. Updates 12500. Training 69.4% complete. Critic_loss: -1.113. Gen_loss: 0.002. Regularizer: 0.043\n",
      "Epoch: 699. Updates 12600. Training 69.9% complete. Critic_loss: -1.807. Gen_loss: -0.364. Regularizer: 0.044\n",
      "Epoch: 705. Updates 12700. Training 70.5% complete. Critic_loss: -1.624. Gen_loss: 0.072. Regularizer: 0.107\n",
      "Epoch: 711. Updates 12800. Training 71.1% complete. Critic_loss: -1.568. Gen_loss: 0.086. Regularizer: 0.037\n",
      "Epoch: 716. Updates 12900. Training 71.6% complete. Critic_loss: -0.993. Gen_loss: 0.395. Regularizer: 0.058\n",
      "Epoch: 722. Updates 13000. Training 72.2% complete. Critic_loss: -1.185. Gen_loss: -0.176. Regularizer: 0.035\n",
      "Epoch: 727. Updates 13100. Training 72.7% complete. Critic_loss: -1.315. Gen_loss: 0.365. Regularizer: 0.046\n",
      "Epoch: 733. Updates 13200. Training 73.3% complete. Critic_loss: -1.619. Gen_loss: 0.565. Regularizer: 0.080\n",
      "Epoch: 738. Updates 13300. Training 73.8% complete. Critic_loss: -0.825. Gen_loss: 0.055. Regularizer: 0.039\n",
      "Epoch: 744. Updates 13400. Training 74.4% complete. Critic_loss: -1.537. Gen_loss: -0.332. Regularizer: 0.049\n",
      "Epoch: 749. Updates 13500. Training 74.9% complete. Critic_loss: -2.099. Gen_loss: 0.303. Regularizer: 0.072\n",
      "Epoch: 755. Updates 13600. Training 75.5% complete. Critic_loss: -1.748. Gen_loss: -2.450. Regularizer: 0.049\n",
      "Epoch: 761. Updates 13700. Training 76.1% complete. Critic_loss: -1.821. Gen_loss: -0.549. Regularizer: 0.028\n",
      "Epoch: 766. Updates 13800. Training 76.6% complete. Critic_loss: -1.570. Gen_loss: -1.469. Regularizer: 0.054\n",
      "Epoch: 772. Updates 13900. Training 77.2% complete. Critic_loss: -1.631. Gen_loss: 1.014. Regularizer: 0.026\n",
      "Epoch: 777. Updates 14000. Training 77.7% complete. Critic_loss: -0.140. Gen_loss: 1.586. Regularizer: 0.313\n",
      "Epoch: 783. Updates 14100. Training 78.3% complete. Critic_loss: -1.461. Gen_loss: 1.348. Regularizer: 0.023\n",
      "Epoch: 788. Updates 14200. Training 78.8% complete. Critic_loss: -1.871. Gen_loss: 1.103. Regularizer: 0.074\n",
      "Epoch: 794. Updates 14300. Training 79.4% complete. Critic_loss: -1.568. Gen_loss: 0.097. Regularizer: 0.000\n",
      "Epoch: 799. Updates 14400. Training 79.9% complete. Critic_loss: -1.620. Gen_loss: 0.371. Regularizer: 0.107\n",
      "Epoch: 805. Updates 14500. Training 80.5% complete. Critic_loss: -1.569. Gen_loss: -0.744. Regularizer: 0.022\n",
      "Epoch: 811. Updates 14600. Training 81.1% complete. Critic_loss: -1.584. Gen_loss: 0.727. Regularizer: 0.023\n",
      "Epoch: 816. Updates 14700. Training 81.6% complete. Critic_loss: -1.202. Gen_loss: -0.430. Regularizer: 0.115\n",
      "Epoch: 822. Updates 14800. Training 82.2% complete. Critic_loss: -1.371. Gen_loss: 0.038. Regularizer: 0.004\n",
      "Epoch: 827. Updates 14900. Training 82.7% complete. Critic_loss: -1.857. Gen_loss: -0.589. Regularizer: 0.045\n",
      "Epoch: 833. Updates 15000. Training 83.3% complete. Critic_loss: -1.282. Gen_loss: -0.931. Regularizer: 0.007\n",
      "Epoch: 838. Updates 15100. Training 83.8% complete. Critic_loss: -1.752. Gen_loss: -0.088. Regularizer: 0.099\n",
      "Epoch: 844. Updates 15200. Training 84.4% complete. Critic_loss: -1.503. Gen_loss: -0.366. Regularizer: 0.019\n",
      "Epoch: 849. Updates 15300. Training 84.9% complete. Critic_loss: -1.874. Gen_loss: -1.722. Regularizer: 0.039\n",
      "Epoch: 855. Updates 15400. Training 85.5% complete. Critic_loss: -1.668. Gen_loss: -0.485. Regularizer: 0.023\n",
      "Epoch: 861. Updates 15500. Training 86.1% complete. Critic_loss: -1.711. Gen_loss: -1.556. Regularizer: 0.002\n",
      "Epoch: 866. Updates 15600. Training 86.6% complete. Critic_loss: -1.146. Gen_loss: 0.317. Regularizer: 0.092\n",
      "Epoch: 872. Updates 15700. Training 87.2% complete. Critic_loss: -1.565. Gen_loss: 0.328. Regularizer: 0.007\n",
      "Epoch: 877. Updates 15800. Training 87.7% complete. Critic_loss: -1.684. Gen_loss: 1.736. Regularizer: 0.153\n",
      "Epoch: 883. Updates 15900. Training 88.3% complete. Critic_loss: -1.253. Gen_loss: 0.366. Regularizer: 0.018\n",
      "Epoch: 888. Updates 16000. Training 88.8% complete. Critic_loss: -1.339. Gen_loss: -0.093. Regularizer: 0.116\n",
      "Epoch: 894. Updates 16100. Training 89.4% complete. Critic_loss: -1.462. Gen_loss: 0.020. Regularizer: 0.007\n",
      "Epoch: 899. Updates 16200. Training 89.9% complete. Critic_loss: -1.902. Gen_loss: 0.001. Regularizer: 0.396\n",
      "Epoch: 905. Updates 16300. Training 90.5% complete. Critic_loss: -1.373. Gen_loss: -0.042. Regularizer: 0.022\n",
      "Epoch: 911. Updates 16400. Training 91.1% complete. Critic_loss: -1.388. Gen_loss: -0.838. Regularizer: 0.005\n",
      "Epoch: 916. Updates 16500. Training 91.6% complete. Critic_loss: -1.555. Gen_loss: -0.674. Regularizer: 0.133\n",
      "Epoch: 922. Updates 16600. Training 92.2% complete. Critic_loss: -1.517. Gen_loss: -0.531. Regularizer: 0.023\n",
      "Epoch: 927. Updates 16700. Training 92.7% complete. Critic_loss: -1.693. Gen_loss: -0.195. Regularizer: 0.117\n",
      "Epoch: 933. Updates 16800. Training 93.3% complete. Critic_loss: -1.389. Gen_loss: -0.087. Regularizer: 0.013\n",
      "Epoch: 938. Updates 16900. Training 93.8% complete. Critic_loss: -1.601. Gen_loss: -0.396. Regularizer: 0.050\n",
      "Epoch: 944. Updates 17000. Training 94.4% complete. Critic_loss: -1.379. Gen_loss: -0.449. Regularizer: 0.001\n",
      "Epoch: 949. Updates 17100. Training 94.9% complete. Critic_loss: -2.122. Gen_loss: -0.758. Regularizer: 0.065\n",
      "Epoch: 955. Updates 17200. Training 95.5% complete. Critic_loss: -1.658. Gen_loss: 0.290. Regularizer: 0.055\n",
      "Epoch: 961. Updates 17300. Training 96.1% complete. Critic_loss: -1.669. Gen_loss: -0.475. Regularizer: 0.013\n",
      "Epoch: 966. Updates 17400. Training 96.6% complete. Critic_loss: -1.069. Gen_loss: 0.316. Regularizer: 0.140\n",
      "Epoch: 972. Updates 17500. Training 97.2% complete. Critic_loss: -1.244. Gen_loss: 0.020. Regularizer: 0.005\n",
      "Epoch: 977. Updates 17600. Training 97.7% complete. Critic_loss: -1.710. Gen_loss: 0.301. Regularizer: 0.079\n",
      "Epoch: 983. Updates 17700. Training 98.3% complete. Critic_loss: -1.086. Gen_loss: -1.180. Regularizer: 0.017\n",
      "Epoch: 988. Updates 17800. Training 98.8% complete. Critic_loss: -1.851. Gen_loss: -0.050. Regularizer: 0.092\n",
      "Epoch: 994. Updates 17900. Training 99.4% complete. Critic_loss: -1.495. Gen_loss: -0.985. Regularizer: 0.001\n",
      "Epoch: 999. Updates 18000. Training 99.9% complete. Critic_loss: -1.855. Gen_loss: -0.557. Regularizer: 0.063\n"
     ]
    }
   ],
   "source": [
    "# Train WGAN and save losses of the model\n",
    "# name_file = \"train_10\"\n",
    "# losses_10 = wass_gan.train(X_train[9], name_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmvx8QFFGFTY"
   },
   "outputs": [],
   "source": [
    "# Plot the graph of the model loss\n",
    "def plot_graph(losses):\n",
    "  disc_step_loss = losses['critic']\n",
    "  gen_step_loss = losses['generator']\n",
    "  reg_step_loss = losses['regularizer']\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.ylim(-30,30)\n",
    "  plt.plot(gen_step_loss, label=\"Generator Loss\")\n",
    "  plt.plot(disc_step_loss, label=\"Discriminator Loss\")\n",
    "  plt.plot(reg_step_loss, label=\"Regularizer Loss\")\n",
    "  plt.grid(True, \"both\", \"both\")\n",
    "  plt.legend()\n",
    "  plt.savefig('graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6KokvIWGMOT"
   },
   "outputs": [],
   "source": [
    "def generate_new_image(gan):\n",
    "  # Generate a new sample from the generator model from the latent space\n",
    "  new_image = gan.generate(1)                         # Generate 1 new image\n",
    "  new_image = np.array(new_image).reshape((128,128))   # Resize and convert into an array the image from the generator\n",
    "  plt.imshow(new_image,cmap='gray')                    # Show and plot the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6U8dVYQ3VzM4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMXjiH25ouvWGV8tLF1ovcY",
   "collapsed_sections": [],
   "name": "wgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
